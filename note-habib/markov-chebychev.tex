% Created 2022-12-23 Fri 13:49
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{cancel}
\author{Biggus Diccus}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={Biggus Diccus},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 28.1 (Org mode 9.5.5)}, 
 pdflang={English}}
\begin{document}

\tableofcontents

\section{Disuguaglianza di Markov}
\label{sec:orge3e98d1}
lode a \url{https://en.wikipedia.org/wiki/Markov\%27s\_inequality}

La disuguaglianza di Markov (che in realtà ha pubblicato prima Chebychev) afferma che data
\begin{itemize}
\item X una V.A. non negativa per cui
\begin{itemize}
\item \(\exists \mathbb{E}[X]\)
\end{itemize}
\end{itemize}
allora
\[\mathbb{P}(X \geq \alpha) \leq \frac{\mathbb{E}[X]}{\alpha}\]

\subsection{Dimostrazione}
\label{sec:org78590db}
Partendo dalla definizione di \(\mathbb{E}[X]\)

\begin{align*}
\mathbb{E}[X] &= \int_{\mathbb{R}} xf(x)dx \text{, ma $X$ non negativa, quindi} \\
&= \int_{0}^{+ \infty} xf(x) dx \\
&= \int_{0}^{\alpha} xf(x) dx + \int_{\alpha}^{+ \infty} xf(x) dx \\
&\geq \int_{\alpha}^{+ \infty} xf(x) dx \\
&\geq \int_{\alpha}^{+ \infty} \alpha f(x) dx \\
&= \alpha \int_{\alpha}^{+ \infty} f(x) dx \text{, che per definizione di $f(x)$} \\
&= \alpha \mathbb{P}(X \in [\alpha, + \infty]) \\
&= \alpha \mathbb{P}(X \geq \alpha) \\
\end{align*}

da questo puttanaio si arriva a
\[ \mathbb{E}[X] \geq \alpha \mathbb{P}(X \geq \alpha) \]

da cui si ricava abbastanza facilmente che

\[ \mathbb{P}(X \geq \alpha) \leq \frac{\mathbb{E}[X]}{\alpha} \]

\section{Chebychev}
\label{sec:org54e5aff}
La disuguaglianza di Chebychev da un limite a quanto può variare dalla media una V.A. data
la sua varianza \footnote{perchè la gente si diverte male}

essa afferma che, dati
\begin{itemize}
\item X una V.A. per cui
\begin{itemize}
\item \(\exists \text{ finito } \mathbb{E}[X]\)
\end{itemize}
\item \(\mu\) la media di \(X\)
\item \(\sigma ^2\) la varianza di x \(\iff\) \(\sigma\) la deviazione standard di \(X\)
\item un qualche \(k\) a caso \(\in \mathbb{R}\), con \(k \geq 0\)
\end{itemize}
allora

\[\mathbb{P} (\lvert X - \mu \rvert \geq k\sigma) \leq \frac{1}{k^2} \]

\subsection{Dimostrazione}
\label{sec:org8c4ae59}
La disuguaglianza di Chebychev si può dimostrare come "corollario" di quella di Markov,
questo si fa facendo gli stronzi con Markov come segue :

da Markov sappiamo che \(\forall\ va\ X\) e \(\alpha \in \mathbb{R}\) allora
\[ \mathbb{P}(X \geq \alpha) \leq \frac{\mathbb{E}[X]}{\alpha} \]
visto che sta cosa vale \(\forall\ va\ X , \alpha \in \mathbb{R}\) allora facciamo
l'adattatore di tacchini per Markov ponendo
\begin{itemize}
\item \(Y = {(X - \mu)}^2\)
\item \(\alpha = {(k\sigma)}^2\)
\end{itemize}

date queste scelte \footnote{fatte palesemente per distruggere il pianeta a semplificazioni}
si arriva a
\[ \mathbb{P}(Y \geq {(k\sigma)}^2) \leq \frac{\mathbb{E}[Y]}{{(k\sigma)}^2} \]

massacriamo ora il primo termine, e poi il secondo
\subsubsection{Primo}
\label{sec:org9037a8b}
\begin{align*}
&\mathbb{P}(Y \geq {(k\sigma)}^2)\\
= &\mathbb{P}({(X - \mu)}^2 \geq {(k\sigma)}^2)
\end{align*}

visto che sia \({(X - \mu)}^2\) che \({(k\sigma)}^2\) sono positivi, allora possiamo mettere la
radice da entrambe le parti della disequanzione, ottenendo

\begin{align*}
& \mathbb{P}({(X - \mu)}^2 \geq {(k\sigma)}^2) \\
= &\mathbb{P}(\sqrt{{(X - \mu)}^2} \geq \sqrt{{(k\sigma)}^2}) \\
= &\mathbb{P}(\lvert (X - \mu) \rvert \geq \lvert k\sigma \rvert)
\end{align*}

visto che sia \(k\)(ipotesi) che \(\sigma\)(definizione) sono positivi, allora \(k\sigma \geq 0\),
quindi

\[ = \mathbb{P}(\lvert (X - \mu) \rvert \geq k\sigma) \]

\subsubsection{Secondo}
\label{sec:org2e9ed62}
\begin{align*}
&\frac{\mathbb{E}[Y]}{{(k\sigma)}^2} \\
= &\frac{\mathbb{E}[{(X - \mu)}^2]}{{(k\sigma)}^2}
\end{align*}

Visto che \(Var[X] = \sigma^2\) si può definire anche come
\[\sigma^2 = \mathbb{E}[{(X - \mathbb{E}[X])}^2] = \mathbb{E}[{(X - \mu)}^2] \]

allora

\begin{align*}
& \frac{\mathbb{E}[{(X - \mu)}^2]}{{(k\sigma)}^2} \\
&= \frac{\sigma^2}{{(k\sigma)}^2} \\
&= \frac{\cancel{\sigma^2}}{k^2 \cancel{\sigma^2}} = \frac{1}{k^2}
\end{align*}

\subsubsection{Metti insieme}
\label{sec:orgaa8040a}

\[ \mathbb{P}(Y \geq {(k\sigma)}^2) \leq \frac{\mathbb{E}[Y]}{{(k\sigma)}^2} \]
riscrivendo il primo termine
\[ \mathbb{P}(\lvert (X - \mu) \rvert \geq \lvert k\sigma \rvert)
\leq \frac{\mathbb{E}[Y]}{{(k\sigma)}^2} \]
e riscrivendo il secondo termine
\[ \mathbb{P}(\lvert (X - \mu) \rvert \geq \lvert k\sigma \rvert) \leq \frac{1}{{k^2}} \]

\(\mathbb{Q.E.D.}\), puppa
\end{document}