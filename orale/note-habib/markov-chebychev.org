#+LATEX_HEADER:\usepackage{cancel}
* Disuguaglianza di Markov
lode a https://en.wikipedia.org/wiki/Markov%27s_inequality

La disuguaglianza di Markov (che in realtà ha pubblicato prima Chebychev) afferma che data
 - X una V.A. non negativa per cui
   - $\exists \mathbb{E}[X]$
allora
\[\mathbb{P}(X \geq \alpha) \leq \frac{\mathbb{E}[X]}{\alpha}\]

** Dimostrazione
Partendo dalla definizione di $\mathbb{E}[X]$

\begin{align*}
\mathbb{E}[X] &= \int_{\mahtbb{R}} xf(x)dx \text{, ma $X$ non negativa, quindi} \\
&= \int_{0}^{+ \infty} xf(x) dx \\
&= \int_{0}^{\alpha} xf(x) dx + \int_{\alpha}^{+ \infty} xf(x) dx \\
&\geq \int_{\alpha}^{+ \infty} xf(x) dx \\
&\geq \int_{\alpha}^{+ \infty} \alpha f(x) dx \\
&= \alpha \int_{\alpha}^{+ \infty} f(x) dx \text{, che per definizione di $f(x)$} \\
&= \alpha \mathbb{P}(X \in [\alpha, + \infty]) \\
&= \alpha \mathbb{P}(X \geq \alpha) \\
\end{align*}

da questo puttanaio si arriva a
\[ \mathbb{E}[X] \geq \alpha \mathbb{P}(X \geq \alpha) \]

da cui si ricava abbastanza facilmente che

\[ \mathbb{P}(X \geq \alpha) \leq \frac{\mathbb{E}[X]}{\alpha} \]

* Chebychev
La disuguaglianza di Chebychev da un limite a quanto può variare dalla media una V.A. data
la sua varianza [fn::perchè la gente si diverte male]

essa afferma che, dati
 - X una V.A. per cui
   - $\exists \text{ finito } \mathbb{E}[X]$
 - $\mu$ la media di $X$
 - $\sigma ^2$ la varianza di x $\iff$ $\sigma$ la deviazione standard di $X$
 - un qualche $k$ a caso $\in \mathbb{R}$, con $k \geq 0$
allora

\[\mathbb{P} (\lvert X - \mu \rvert \geq k\sigma) \leq \frac{1}{k^2} \]

** Dimostrazione
La disuguaglianza di Chebychev si può dimostrare come "corollario" di quella di Markov,
questo si fa facendo gli stronzi con Markov come segue :

da Markov sappiamo che $\forall\ va\ X$ e $\alpha \in \mathbb{R}$ allora
\[ \mathbb{P}(X \geq \alpha) \leq \frac{\mathbb{E}[X]}{\alpha} \]
visto che sta cosa vale $\forall\ va\ X , \alpha \in \mathbb{R}$ allora facciamo
l'adattatore di tacchini per Markov ponendo
 - $Y = {(X - \mu)}^2$
 - $\alpha = {(k\sigma)}^2$

date queste scelte [fn::fatte palesemente per distruggere il pianeta a semplificazioni]
si arriva a
\[ \mathbb{P}(Y \geq {(k\sigma)}^2) \leq \frac{\mathbb{E}[Y]}{{(k\sigma)}^2} \]

massacriamo ora il primo termine, e poi il secondo
*** Primo
\begin{align*}
&\mathbb{P}(Y \geq {(k\sigma)}^2)\\
= &\mathbb{P}({(X - \mu)}^2 \geq {(k\sigma)}^2)
\end{align*}

visto che sia ${(X - \mu)}^2$ che ${(k\sigma)}^2$ sono positivi, allora possiamo mettere la
radice da entrambe le parti della disequanzione, ottenendo

\begin{align*}
& \mathbb{P}({(X - \mu)}^2 \geq {(k\sigma)}^2) \\
= &\mathbb{P}(\sqrt{{(X - \mu)}^2} \geq \sqrt{{(k\sigma)}^2}) \\
= &\mathbb{P}(\lvert (X - \mu) \rvert \geq \lvert k\sigma \rvert)
\end{align*}

visto che sia $k$(ipotesi) che $\sigma$(definizione) sono positivi, allora $k\sigma \geq 0$,
quindi

\[ = \mathbb{P}(\lvert (X - \mu) \rvert \geq k\sigma) \]

*** Secondo
\begin{align*}
&\frac{\mathbb{E}[Y]}{{(k\sigma)}^2} \\
= &\frac{\mathbb{E}[{(X - \mu)}^2]}{{(k\sigma)}^2}
\end{align*}

Visto che $Var[X] = \sigma^2$ si può definire anche come
\[\sigma^2 = \mathbb{E}[{(X - \mathbb{E}[X])}^2] = \mathbb{E}[{(X - \mu)}^2] \]

allora

\begin{align*}
& \frac{\mathbb{E}[{(X - \mu)}^2]}{{(k\sigma)}^2} \\
&= \frac{\sigma^2}{{(k\sigma)}^2} \\
&= \frac{\cancel{\sigma^2}}{k^2 \cancel{\sigma^2}} = \frac{1}{k^2}
\end{align*}

*** Metti insieme

\[ \mathbb{P}(Y \geq {(k\sigma)}^2) \leq \frac{\mathbb{E}[Y]}{{(k\sigma)}^2} \]
riscrivendo il primo termine
\[ \mathbb{P}(\lvert (X - \mu) \rvert \geq \lvert k\sigma \rvert)
\leq \frac{\mathbb{E}[Y]}{{(k\sigma)}^2} \]
e riscrivendo il secondo termine
\[ \mathbb{P}(\lvert (X - \mu) \rvert \geq \lvert k\sigma \rvert) \leq \frac{1}{{k^2} \]

$\mathbb{Q.E.D.}$, puppa
